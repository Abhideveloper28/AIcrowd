ID,Challenge Name,Paper Link,AIcrowd Contributed,Title,Author,Publication Date,Venue,Citation,Abstract,Tags,Cite,Featured
,https://www.aicrowd.com/challenges/neurips-2021-minerl-basalt-competition,https://arxiv.org/abs/2107.01969,,The MineRL BASALT Competition on Learning from Human Feedback,"Rohin Shah, Cody Wild, Steven H. Wang, Neel Alex, Brandon Houghton, William Guss, Sharada Mohanty, Anssi Kanervisto, Stephanie Milani, Nicholay Topin, Pieter Abbeel, Stuart Russell, Anca Dragan

",07/05/21,NeurIPS 2021 Competition Track,,"The last decade has seen a significant increase of interest in deep learning research, with many public successes that have demonstrated its potential. As such, these systems are now being incorporated into commercial products. With this comes an additional challenge: how can we build AI systems that solve tasks where there is not a crisp, well-defined specification? While multiple solutions have been proposed, in this competition we focus on one in particular: learning from human feedback. Rather than training AI systems using a predefined reward function or using a labeled dataset with a predefined set of categories, we instead train the AI system using a learning signal derived from some form of human feedback, which can evolve over time as the understanding of the task changes, or as the capabilities of the AI system improve.
The MineRL BASALT competition aims to spur forward research on this important class of techniques. We design a suite of four tasks in Minecraft for which we expect it will be hard to write down hardcoded reward functions. These tasks are defined by a paragraph of natural language: for example, ""create a waterfall and take a scenic picture of it"", with additional clarifying details. Participants must train a separate agent for each task, using any method they want. Agents are then evaluated by humans who have read the task description. To help participants get started, we provide a dataset of human demonstrations on each of the four tasks, as well as an imitation learning baseline that leverages these demonstrations.
Our hope is that this competition will improve our ability to build AI systems that do what their designers intend them to do, even when the intent cannot be easily formalized. Besides allowing AI to solve more tasks, this can also enable more effective regulation of AI systems, as well as making progress on the value alignment problem. ",,"@misc{shah2021minerl,
      title={The MineRL BASALT Competition on Learning from Human Feedback}, 
      author={Rohin Shah and Cody Wild and Steven H. Wang and Neel Alex and Brandon Houghton and William Guss and Sharada Mohanty and Anssi Kanervisto and Stephanie Milani and Nicholay Topin and Pieter Abbeel and Stuart Russell and Anca Dragan},
      year={2021},
      eprint={2107.01969},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}",
,aicrowd.com/challenges/food-recognition-challenge,https://arxiv.org/abs/2106.14977,,The Food Recognition Benchmark: Using DeepLearning to Recognize Food on Images,"Sharada Prasanna Mohanty, Gaurav Singhal, Eric Antoine Scuccimarra, Djilani Kebaili, Harris Héritier, Victor Boulanger, Marcel Salathé",2021/6/28,arXiv,,"The automatic recognition of food on images has numerous interesting applications, including nutritional tracking in medical cohorts. The problem has received significant research attention, but an ongoing public benchmark to develop open and reproducible algorithms has been missing. Here, we report on the setup of such a benchmark using publicly available food images sourced through the mobile MyFoodRepo app. Through four rounds, the benchmark released the MyFoodRepo-273 dataset constituting 24,119 images and a total of 39,325 segmented polygons categorized in 273 different classes. Models were evaluated on private tests sets from the same platform with 5,000 images and 7,865 annotations in the final round. Top-performing models on the 273 food categories reached a mean average precision of 0.568 (round 4) and a mean average recall of 0.885 (round 3). We present experimental validation of round 4 results, and discuss implications of the benchmark setup designed to increase the size and diversity of the dataset for future rounds.",,"@misc{mohanty2021food,
      title={The Food Recognition Benchmark: Using DeepLearning to Recognize Food on Images}, 
      author={Sharada Prasanna Mohanty and Gaurav Singhal and Eric Antoine Scuccimarra and Djilani Kebaili and Harris Héritier and Victor Boulanger and Marcel Salathé},
      year={2021},
      eprint={2106.14977},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}",
,https://www.aicrowd.com/challenges/neurips-2021-minerl-diamond-competition,https://arxiv.org/abs/2106.03748,,Towards robust and domain agnostic reinforcement learning competitions,"William Hebgen Guss, Stephanie Milani, Nicholay Topin, Brandon Houghton, Sharada Mohanty, Andrew Melnik, Augustin Harter, Benoit Buschmaas, Bjarne Jaster, Christoph Berganski, Dennis Heitkamp, Marko Henning, Helge Ritter, Chengjie Wu, Xiaotian Hao, Yiming Lu, Hangyu Mao, Yihuan Mao, Chao Wang, Michal Opanowicz, Anssi Kanervisto, Yanick Schraner, Christian Scheller, Xiren Zhou, Lu Liu, Daichi Nishio, Toi Tsuneda, Karolis Ramanauskas, Gabija Juceviciute",2021/6/7,arXiv,,"     Reinforcement learning competitions have formed the basis for standard research benchmarks, galvanized advances in the state-of-the-art, and shaped the direction of the field. Despite this, a majority of challenges suffer from the same fundamental problems: participant solutions to the posed challenge are usually domain-specific, biased to maximally exploit compute resources, and not guaranteed to be reproducible. In this paper, we present a new framework of competition design that promotes the development of algorithms that overcome these barriers. We propose four central mechanisms for achieving this end: submission retraining, domain randomization, desemantization through domain obfuscation, and the limitation of competition compute and environment-sample budget. To demonstrate the efficacy of this design, we proposed, organized, and ran the MineRL 2020 Competition on Sample-Efficient Reinforcement Learning. In this work, we describe the organizational outcomes of the competition and show that the resulting participant submissions are reproducible, non-specific to the competition environment, and sample/resource efficient, despite the difficult competition task. ",,"@misc{guss2021robust,
      title={Towards robust and domain agnostic reinforcement learning competitions}, 
      author={William Hebgen Guss and Stephanie Milani and Nicholay Topin and Brandon Houghton and Sharada Mohanty and Andrew Melnik and Augustin Harter and Benoit Buschmaas and Bjarne Jaster and Christoph Berganski and Dennis Heitkamp and Marko Henning and Helge Ritter and Chengjie Wu and Xiaotian Hao and Yiming Lu and Hangyu Mao and Yihuan Mao and Chao Wang and Michal Opanowicz and Anssi Kanervisto and Yanick Schraner and Christian Scheller and Xiren Zhou and Lu Liu and Daichi Nishio and Toi Tsuneda and Karolis Ramanauskas and Gabija Juceviciute},
      year={2021},
      eprint={2106.03748},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}",
,https://www.aicrowd.com/challenges/snakeclef2021-snake-species-identification-challenge,https://www.frontiersin.org/articles/10.3389/frai.2021.582110/full?utm_source=S-TWT&utm_medium=SNET&utm_campaign=ECO_FRAI_FDATA_XXXXXXXX_auto-dlvrit,,Supervised Learning Computer Vision Benchmark for Snake Species Identification From Photographs: Implications for Herpetology and Global Health,"Andrew M Durso, Gokula Krishnan Moorthy, Sharada P Mohanty, Isabelle Bolon, Marcel Salathé, Rafael Ruiz De Castañeda",2021/4/20,Frontiers,5,"We trained a computer vision algorithm to identify 45 species of snakes from photos and compared its performance to that of humans. Both human and algorithm performance is substantially better than randomly guessing (null probability of guessing correctly given 45 classes = 2.2%). Some species (e.g. Boa constrictor) are routinely identified with ease by both algorithm and humans, whereas other groups of species (e.g. uniform green snakes, blotched brown snakes) are routinely confused. A species complex with largely molecular species delimitation (North American ratsnakes) was the most challenging for computer vision. Humans had an edge at identifying images of poor quality or with visual artifacts. With future improvement, computer vision could play a larger role in snakebite epidemiology, particularly when combined with information about geographic location and input from human experts.",,,
,https://www.aicrowd.com/challenges/multi-agent-behavior-representation-modeling-measurement-and-applications,https://arxiv.org/abs/2104.02710,,The Multi-Agent Behavior Dataset: Mouse Dyadic Social Interactions,"Jennifer J Sun, Tomomi Karigo, Dipam Chakraborty, Sharada P Mohanty, David J Anderson, Pietro Perona, Yisong Yue, Ann Kennedy",2021/4/6,arXiv,1,"Multi-agent behavior modeling aims to understand the interactions that occur between agents. We present a multi-agent dataset from behavioral neuroscience, the Caltech Mouse Social Interactions (CalMS21) Dataset. Our dataset consists of trajectory data of social interactions, recorded from videos of freely behaving mice in a standard resident-intruder assay. The CalMS21 dataset is part of the Multi-Agent Behavior Challenge 2021 and for our next step, our goal is to incorporate datasets from other domains studying multi-agent behavior. To help accelerate behavioral studies, the CalMS21 dataset provides a benchmark to evaluate the performance of automated behavior classification methods in three settings: (1) for training on large behavioral datasets all annotated by a single annotator, (2) for style transfer to learn inter-annotator differences in behavior definitions, and (3) for learning of new behaviors of interest given limited training data. The dataset consists of 6 million frames of unlabelled tracked poses of interacting mice, as well as over 1 million frames with tracked poses and corresponding frame-level behavior annotations. The challenge of our dataset is to be able to classify behaviors accurately using both labelled and unlabelled tracking data, as well as being able to generalize to new annotators and behaviors.",,"@misc{sun2021multiagent,
      title={The Multi-Agent Behavior Dataset: Mouse Dyadic Social Interactions}, 
      author={Jennifer J. Sun and Tomomi Karigo and Dipam Chakraborty and Sharada P. Mohanty and Benjamin Wild and Quan Sun and Chen Chen and David J. Anderson and Pietro Perona and Yisong Yue and Ann Kennedy},
      year={2021},
      eprint={2104.02710},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}",
,"https://www.aicrowd.com/challenges/flatland-challenge
",https://arxiv.org/abs/2103.16511,,Flatland Competition 2020: MAPF and MARL for Efficient Train Coordination on a Grid World,"Florian Laurent, Manuel Schneider, Christian Scheller, Jeremy Watson, Jiaoyang Li, Zhe Chen, Yi Zheng, Shao-Hung Chan, Konstantin Makhnev, Oleg Svidchenko, Vladimir Egorov, Dmitry Ivanov, Aleksei Shpilman, Evgenija Spirovska, Oliver Tanevski, Aleksandar Nikov, Ramon Grunder, David Galevski, Jakov Mitrovski, Guillaume Sartoretti, Zhiyao Luo, Mehul Damani, Nilabha Bhattacharya, Shivam Agarwal, Adrian Egli, Erik Nygren, Sharada Mohanty",2021/3/30,arXiv,3,"     The Flatland competition aimed at finding novel approaches to solve the vehicle re-scheduling problem (VRSP). The VRSP is concerned with scheduling trips in traffic networks and the re-scheduling of vehicles when disruptions occur, for example the breakdown of a vehicle. While solving the VRSP in various settings has been an active area in operations research (OR) for decades, the ever-growing complexity of modern railway networks makes dynamic real-time scheduling of traffic virtually impossible. Recently, multi-agent reinforcement learning (MARL) has successfully tackled challenging tasks where many agents need to be coordinated, such as multiplayer video games. However, the coordination of hundreds of agents in a real-life setting like a railway network remains challenging and the Flatland environment used for the competition models these real-world properties in a simplified manner. Submissions had to bring as many trains (agents) to their target stations in as little time as possible. While the best submissions were in the OR category, participants found many promising MARL approaches. Using both centralized and decentralized learning based approaches, top submissions used graph representations of the environment to construct tree-based observations. Further, different coordination mechanisms were implemented, such as communication and prioritization between agents. This paper presents the competition setup, four outstanding solutions to the competition, and a cross-comparison between them. ",,"@misc{laurent2021flatland,
      title={Flatland Competition 2020: MAPF and MARL for Efficient Train Coordination on a Grid World}, 
      author={Florian Laurent and Manuel Schneider and Christian Scheller and Jeremy Watson and Jiaoyang Li and Zhe Chen and Yi Zheng and Shao-Hung Chan and Konstantin Makhnev and Oleg Svidchenko and Vladimir Egorov and Dmitry Ivanov and Aleksei Shpilman and Evgenija Spirovska and Oliver Tanevski and Aleksandar Nikov and Ramon Grunder and David Galevski and Jakov Mitrovski and Guillaume Sartoretti and Zhiyao Luo and Mehul Damani and Nilabha Bhattacharya and Shivam Agarwal and Adrian Egli and Erik Nygren and Sharada Mohanty},
      year={2021},
      eprint={2103.16511},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}",
,https://www.aicrowd.com/challenges/neurips-2020-procgen-competition,"https://arxiv.org/abs/2103.15332
",,"Measuring Sample Efficiency and Generalization in Reinforcement Learning Benchmarks: NeurIPS 2020 Procgen Benchmark
","Sharada Mohanty, Jyotish Poonganam, Adrien Gaidon, Andrey Kolobov, Blake Wulfe, Dipam Chakraborty, Gražvydas Šemetulskis, João Schapke, Jonas Kubilius, Jurgis Pašukonis, Linas Klimas, Matthew Hausknecht, Patrick MacAlpine, Quang Nhat Tran, Thomas Tumiel, Xiaocheng Tang, Xinwei Chen, Christopher Hesse, Jacob Hilton, William Hebgen Guss, Sahika Genc, John Schulman, Karl Cobbe
",2021/3/29,arXiv,2,"The NeurIPS 2020 Procgen Competition was designed as a centralized benchmark with clearly defined tasks for measuring Sample Efficiency and Generalization in Reinforcement Learning. Generalization remains one of the most fundamental challenges in deep reinforcement learning, and yet we do not have enough benchmarks to measure the progress of the community on Generalization in Reinforcement Learning. We present the design of a centralized benchmark for Reinforcement Learning which can help measure Sample Efficiency and Generalization in Reinforcement Learning by doing end to end evaluation of the training and rollout phases of thousands of user submitted code bases in a scalable way. We designed the benchmark on top of the already existing Procgen Benchmark by defining clear tasks and standardizing the end to end evaluation setups. The design aims to maximize the flexibility available for researchers who wish to design future iterations of such benchmarks, and yet imposes necessary practical constraints to allow for a system like this to scale. This paper presents the competition setup and the details and analysis of the top solutions identified through this setup in context of 2020 iteration of the competition at NeurIPS.",,"@misc{mohanty2021measuring,
      title={Measuring Sample Efficiency and Generalization in Reinforcement Learning Benchmarks: NeurIPS 2020 Procgen Benchmark}, 
      author={Sharada Mohanty and Jyotish Poonganam and Adrien Gaidon and Andrey Kolobov and Blake Wulfe and Dipam Chakraborty and Gražvydas Šemetulskis and João Schapke and Jonas Kubilius and Jurgis Pašukonis and Linas Klimas and Matthew Hausknecht and Patrick MacAlpine and Quang Nhat Tran and Thomas Tumiel and Xiaocheng Tang and Xinwei Chen and Christopher Hesse and Jacob Hilton and William Hebgen Guss and Sahika Genc and John Schulman and Karl Cobbe},
      year={2021},
      eprint={2103.15332},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}",
,"https://www.aicrowd.com/challenges/global-wheat-challenge-2021
",https://arxiv.org/abs/2105.07660,,"Global Wheat Head Dataset 2021: more diversity to improve the benchmarking of wheat head localization methods
","Etienne David, Mario Serouart, Daniel Smith, Simon Madec, Kaaviya Velumani, Shouyang Liu, Xu Wang, Francisco Pinto Espinosa, Shahameh Shafiee, Izzat S. A. Tahir, Hisashi Tsujimoto, Shuhei Nasuda, Bangyou Zheng, Norbert Kichgessner, Helge Aasen, Andreas Hund, Pouria Sadhegi-Tehran, Koichi Nagasawa, Goro Ishikawa, Sébastien Dandrifosse, Alexis Carlier, Benoit Mercatoris, Ken Kuroki, Haozhou Wang, Masanori Ishii, Minhajul A. Badhon, Curtis Pozniak, David Shaner LeBauer, Morten Lilimo, Jesse Poland, Scott Chapman, Benoit de Solan, Frédéric Baret, Ian Stavness, Wei Guo",3/6/21,arXiv,,"     The Global Wheat Head Detection (GWHD) dataset was created in 2020 and has assembled 193,634 labelled wheat heads from 4,700 RGB images acquired from various acquisition platforms and 7 countries/institutions. With an associated competition hosted in Kaggle, GWHD has successfully attracted attention from both the computer vision and agricultural science communities. From this first experience in 2020, a few avenues for improvements have been identified, especially from the perspective of data size, head diversity and label reliability. To address these issues, the 2020 dataset has been reexamined, relabeled, and augmented by adding 1,722 images from 5 additional countries, allowing for 81,553 additional wheat heads to be added. We now release a new version of the Global Wheat Head Detection (GWHD) dataset in 2021, which is bigger, more diverse, and less noisy than the 2020 version. The GWHD 2021 is now publicly available at this http URL and a new data challenge has been organized on AIcrowd to make use of this updated dataset. ",,"@misc{david2021global,
      title={Global Wheat Head Dataset 2021: more diversity to improve the benchmarking of wheat head localization methods}, 
      author={Etienne David and Mario Serouart and Daniel Smith and Simon Madec and Kaaviya Velumani and Shouyang Liu and Xu Wang and Francisco Pinto Espinosa and Shahameh Shafiee and Izzat S. A. Tahir and Hisashi Tsujimoto and Shuhei Nasuda and Bangyou Zheng and Norbert Kichgessner and Helge Aasen and Andreas Hund and Pouria Sadhegi-Tehran and Koichi Nagasawa and Goro Ishikawa and Sébastien Dandrifosse and Alexis Carlier and Benoit Mercatoris and Ken Kuroki and Haozhou Wang and Masanori Ishii and Minhajul A. Badhon and Curtis Pozniak and David Shaner LeBauer and Morten Lilimo and Jesse Poland and Scott Chapman and Benoit de Solan and Frédéric Baret and Ian Stavness and Wei Guo},
      year={2021},
      eprint={2105.07660},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}",